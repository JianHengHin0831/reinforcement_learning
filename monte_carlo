import numpy as np

# Define the number of states and actions
num_states = 5
num_actions = 2

# Initialize Q-values and returns
Q = np.zeros((num_states, num_actions))
returns = np.zeros((num_states, num_actions))
N = np.zeros((num_states, num_actions))

# Generate a simple environment with known rewards
true_values = [-10000, -1000, -100, -10, 100000]

# Function to take an action and return a reward
def take_action(state, action):
    if action == 0:  # Left action
        return max(0, state - 1)
    else:  # Right action
        return min(4, state + 1)

# Monte Carlo learning algorithm
num_episodes = 1000

for episode in range(num_episodes):
    states_in_episode = []
    actions_in_episode = []
    rewards = []

    state = np.random.choice(num_states)

    while True:
        action = np.random.choice(np.where(Q[state] == np.max(Q[state]))[0]) if np.random.rand()<0.9 else np.random.choice(num_actions)
        next_state = take_action(state, action)
        reward = true_values[next_state]

        states_in_episode.append(state)
        actions_in_episode.append(action)
        rewards.append(reward)

        if next_state == 4:
            break

        state = next_state

    G = 0
    for t in range(len(states_in_episode) - 1, -1, -1):
        state_t = states_in_episode[t]
        action_t = actions_in_episode[t]
        reward_t = rewards[t]
        G = G + reward_t

        if state_t not in states_in_episode[:t]:
            N[state_t, action_t] += 1
            returns[state_t, action_t] += G
            Q[state_t, action_t] = returns[state_t, action_t] / N[state_t, action_t]


# Display the learned Q-values
print("Learned Q-values:")
print(Q)
