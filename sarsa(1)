import numpy as np

# Define the number of states and actions
num_states = 5
num_actions = 2

# Initialize the Q-table
Q = np.zeros((num_states, num_actions))

# Generate a simple environment with known rewards
true_values = [-100000, -10000, -1000, -100, 1000]

# Function to take an action and return a reward
def take_action(state, action):
    if action == 0:  # Left action
        return max(0, state - 1)
    else:  # Right action
        return min(4, state + 1)

# SARSA(1) learning algorithm
num_episodes = 1000
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.1  # Epsilon-greedy exploration parameter

for episode in range(num_episodes):
    state = np.random.choice(num_states)  # Start from a random state
    action = np.random.choice(num_actions)  # Randomly choose an action

    while True:

      #do for loop
        next_state = take_action(state, action)
        reward = true_values[next_state]

        # Choose the next action using epsilon-greedy policy
        if np.random.rand() < epsilon:
            next_action = np.random.choice(num_actions)
        else:
            next_action = np.argmax(Q[next_state])

        # SARSA(1) update
        next_state_2 = take_action(next_state, next_action)
        if np.random.rand()>epsilon:
          next_action_2 = np.argmax(Q[next_state_2])  # SARSA(1) considers the next step
        else:
          next_action_2 = np.random.choice(num_actions)

        reward_2 = true_values[next_state_2]

        Q[state, action] += alpha * (reward + gamma * (reward_2 + gamma * Q[next_state, next_action] - Q[state, action]))

        if next_state_2 == 4:  # Terminal state reached
            break

        state = next_state_2
        action = next_action_2

# Display the learned Q-values
print("Learned Q-values (SARSA(1)):")
print(Q)
